The Limits of Intelligence: Stuart Russell’s Human Compatible: AI and the Problem of Control (2019)
February 2020

Right now, you have a supercomputer aimed at your head. As you read these words, billions of dollars of digital machinery fights for your most precious resource—your attention. Thankfully, you’re a human. You’re smarter than the even the smartest artificial intelligence that, even now, builds a dossier on you with every click, every keystroke. For now. How will you resist the prod-ding of an algorithm that is smarter than you, knows about you about you than you do, and has instantaneous access to all the worlds’ information? 

Humans have always been the smartest entity in the known universe. And though a scroll through Twitter may give you reason to doubt it, humans remain smarter than our smartest artificial intelli-gence. The world’s most advanced autonomous vehicle system, the Tesla Autopilot, was fooled recently by a two inch piece of tape on a stop sign which appeared to change the speed limit from 35 to 85 miles an hour. Humans would not be likely to make a similar mistake—we can draw from our experience and common sense.

But AIs are catching up to us. Only a few years ago, we were amazed when our AIs began navi-gating for us. We put down our paper maps and allowed our GPS to choose the most efficient route. Sure, ceding navigation duty was rocky at first (see, e.g., Michael Scott driving into a lake on The Office), but our AIs have mostly gotten the hang of navigating. Now, they are taking the wheel. What will we do when AIs are not just taking the wheel but choosing our destination? What happens when AIs are not only superior drivers, but superior decision-makers? Could an algorithm choose my career for me?

In his new book, the preeminent computer scientist Stuart Russell warns that we humans better prepare for the possibility that “machines will far exceed the human capacity for decision making in the real world.” Many scholars aren’t worried about it. Some think superhuman AI is simply im-possible—that human-level intelligence is not platform independent; it requires biological brain matter. This view, says Stuart Russell, is simply “wishful thinking.” After all, “what evidence could there be that no physically possible arrangement of atoms outperforms the human brain?” 

It has always been the goal of AI research, Russell writes, to develop an AI that is not limited to a narrow purpose like driving or winning board games but which can reason and learn as well as humans. An artificial general intelligence (AGI), also called superintelligent AI, would be endowed with a general-purpose problem-solving ability, analogous to the human mind.  An AGI would able to read and understand every book ever written (150 million or so) in a few hours. At the same time, it could monitor every satellite, CCTV, television and radio broadcast, every email and phone conversation taking place all over the globe. Very quickly it would know more about us than we do. Russell reluctantly predicts that humans may build a superintelligence within the lifetime of his children. It would be the single most important event in human history, he says. When it arrives, we’d better be sure we can remain in the driver’s seat.
